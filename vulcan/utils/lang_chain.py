from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain, SequentialChain

from vulcan.configs.llm import LLM

llm = ChatOpenAI(api_key=LLM.OpenAI.OPENAI_API_KEY)


def generate_prompt(template, input_variables):
    """
    Generate a prompt using the specified template and input variables.

    Args:
        template (str): The template for generating the prompt.
        input_variables (list, optional): List of input variables used in the template.

    Returns:
        str: Generated prompt string.
    """
    return PromptTemplate(input_variables=input_variables, template=template)


def get_llm_chain(prompt, output_key):
    """
    Creates and return a Language Model (LLM) chain based on the given prompt and output key.

    Parameters:
    - prompt (str): The input prompt for the language model chain.
    - output_key (str): The key specifying the desired output from the language model.

    Returns:
    LLMChain: An instance of the LLMChain class configured with the provided prompt and output key.
    """
    response = LLMChain(llm=llm, prompt=prompt, output_key=output_key)
    return response


def generate_sequential_chain_response(prompts, input_variables):
    """
    Generate a response using a sequential chain of Language Model (LLM) instances.

    Parameters:
    - prompts (list of str): List of input prompts for each language model in the chain.
    - input_variables (dict): Dictionary mapping input variable names to their values.
    - output_variables (list of str): List of output variable names, one for each language model in the chain.

    Returns:
    Any: The response generated by the sequential chain, corresponding to the final output variable.

    Note:
    This function creates a chain of Language Model (LLM) instances based on the provided prompts and
    output variable names. The prompts are processed sequentially, and the final output is returned.
    """
    output_variables = []
    chains = []
    for prompt in prompts:
        formatted_prompt = generate_prompt(prompt["text"], prompt["input_variables"])
        chains.append(get_llm_chain(formatted_prompt, prompt["output_variable"]))
        output_variables.append(prompt["output_variable"])

    sequential_chain = SequentialChain(
        chains=chains,
        input_variables=list(input_variables.keys()),
        output_variables=output_variables,
    )

    response_dict = sequential_chain(input_variables)
    return response_dict[output_variables[-1]]
